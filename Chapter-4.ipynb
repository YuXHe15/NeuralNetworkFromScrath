{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Softmax Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_{i,j}=\\frac{e^{z_{i,j}}}{\\sum^{L}_{l=1}e^{z_{i,l}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax functions consists of two parts: Exponentiation and Normalization. Exponentiation maps data to a non-negative real number space while keep the class of data. Normalization excludes the influence of the magnitudes among data. Let's create an input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_outputs = [4.8,1.21,2.385] # Values from the earlier previous when we described what a neural network is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the exponential values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      " [121.51041752   3.35348465  10.85906266]\n"
     ]
    }
   ],
   "source": [
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values:\\n',exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And normalized values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized exponentiated values:\n",
      " [0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('normalized exponentiated values:\\n',norm_values)\n",
    "sum_norm_values = np.sum(norm_values)\n",
    "print('sum of normalized values:',sum_norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the next step, test a little more about the np.sum function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array to be summed:\n",
      " [[0.5904968  0.64847236 0.69945019]\n",
      " [0.55927405 0.81059468 0.61866652]\n",
      " [0.4102911  0.21477291 0.56904373]]\n",
      "summed array: 5.121062337742032\n",
      "summed array axis 0: [1.56006195 1.67383995 1.88716044]\n",
      "summed array axis 1: [1.93841934 1.98853526 1.19410774]\n"
     ]
    }
   ],
   "source": [
    "array_to_be_summed = np.random.random((3,3))\n",
    "print('array to be summed:\\n',array_to_be_summed)\n",
    "summed_array = np.sum(array_to_be_summed)\n",
    "print('summed array:',summed_array)\n",
    "summed_array_axis_0 = np.sum(array_to_be_summed,axis=0)\n",
    "print('summed array axis 0:',summed_array_axis_0)\n",
    "summed_array_axis_1 = np.sum(array_to_be_summed,axis=1)\n",
    "print('summed array axis 1:',summed_array_axis_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summation along axis 0 is the summation w.r.t. the columns and axis 1, rows. Now is the time to write the activation function using softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSoftmax:\n",
    "    def forward(self,inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dead Neuron and Large Number\n",
    "Before playing with our new activation function, one more thing need to be mentioned. Generally, very large numbers could \"wreak havoc down\" the line and render a network useless over time, also called a dead neuron. The exponential function used in softmax activation is one of the sources of exploding values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n",
      "22026.465794806718\n",
      "2.6881171418161356e+43\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/r2_r82y54s79c3rhd66b97qm0000gn/T/ipykernel_22438/1740587219.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(1000))\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(1))\n",
    "print(np.exp(10))\n",
    "print(np.exp(100))\n",
    "print(np.exp(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overflow issues can arise even with relatively small numbers. To prevent such errors, we can leverage the property of the exponential function, which tends towards 0 for negative values and equals 1 at 0. By subtracting the largest value from every input, we effectively shift the entire input set along the axis, aligning the largest value with 0. This adjustment ensures that the output of the activation function remains monotonous and avoids explosive behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax  = ActivationSoftmax()\n",
    "softmax.forward([[1,2,3]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather all buddies we knew together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.333359   0.33333178 0.33330921]\n",
      " [0.33338568 0.33333017 0.33328414]\n",
      " [0.33339186 0.3333298  0.33327834]\n",
      " [0.33342196 0.33332798 0.33325006]]\n"
     ]
    }
   ],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class ActivationReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = LayerDense(2,3)\n",
    "activation1 = ActivationReLU()\n",
    "dense2 = LayerDense(3,3)\n",
    "activation2 = ActivationSoftmax()\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "print(activation2.output[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
